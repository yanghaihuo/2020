{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "经典部分回顾：https://mp.weixin.qq.com/s/gAqxE9MA8VPHzaysYLSjsQ\n",
    "利用随机森林预测填补缺失值\n",
    "Age特征缺失值：Age有20%缺失值，缺失值较多，大量删除会减少样本信息，由于它与Cabin不同，这里将利用其它特征进行预测填补Age，也就是拟合未知Age特征值。\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "classers = ['Fare','Parch','Pclass','SibSp','TitleCat',\n",
    "            'CabinCat','female','male', 'Embarked', 'FamilySize', 'NameLength','Ticket_Numbers','Ticket_Id']\n",
    "etr = ExtraTreesRegressor(n_estimators=200,random_state=0)\n",
    "X_train = df[classers][df['Age'].notnull()]\n",
    "Y_train = df['Age'][df['Age'].notnull()]\n",
    "X_test = df[classers][df['Age'].isnull()]\n",
    "\n",
    "etr.fit(X_train.as_matrix(),np.ravel(Y_train))\n",
    "age_preds = etr.predict(X_test.as_matrix())\n",
    "df['Age'][df['Age'].isnull()] = age_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mushrooms[column] = pd.factorize(mushrooms[column])[0] # 返回两个值，取第一个\n",
    "直接得到原始数据的对应的序号列表，将类别信息转化成数值信息应用到模型中去\n",
    "mushrooms[column] =pd.Categorical(mushrooms[column]).codes\n",
    "df['CabinCat'] = pd.Categorical.from_array(df.Cabin.fillna('0').apply(lambda x: x[0])).codes    # pd.Categorical.from_array  numpy数组接口\n",
    "# 量化Embarked特征\n",
    "df[\"Embarked\"] = pd.Categorical.from_array(df.Embarked).codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('训练集的交易记录条数：',X_train.shape[0])\n",
    "print('测试集的交易记录条数：',X_test.shape[0])\n",
    "print('交易记录总数：',X_train.shape[0] + X_test.shape[0])\n",
    "print('上采样前，类别为‘1’的共有{}个，类别为‘0’的共有{}个。'.format(sum(y_train==1),sum(y_train==0)))\n",
    "print('------------------------')\n",
    "\n",
    "# 对训练集进行上采样处理\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_os,y_train_os = smote.fit_sample(X_train, y_train.ravel()) # ravel(): change the shape of y to (n_samples, )\n",
    "\n",
    "print('上采样后，训练集的交易记录条数：', len(X_train_os))\n",
    "print('其中，训练集X的shape:',X_train_os.shape,'，y的shape:',y_train_os.shape)\n",
    "print('交易记录总数：',X_train_os.shape[0] + X_test.shape[0])\n",
    "print('上采样后，类别为‘1’的共有{}个，类别为‘0’的共有{}个。'.format(sum(y_train_os==1),sum(y_train_os==0)))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc, recall_score, classification_report\n",
    "\n",
    "# 定义正则化权重参数，用以控制过拟合\n",
    "paramaters = {'C':np.linspace(1,10, num=10)} # generate sequnce: start = 1, stop = 10\n",
    "paramaters\n",
    "# C_param_range = [0.01,0.1,1,10,100]\n",
    "\n",
    "lr = LogisticRegression()\n",
    "# 5 folds, 3 jobs run in parallel\n",
    "lr_clf = GridSearchCV(lr, paramaters, cv=5, n_jobs=3, verbose=5)\n",
    "lr_clf.fit(X_train_os, y_train_os.ravel())\n",
    "\n",
    "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False),\n",
    "       fit_params=None, iid='warn', n_jobs=3,\n",
    "       param_grid={'C': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=5)\n",
    "\n",
    "print('最好的参数：',lr_clf.best_params_)\n",
    "\n",
    "lr1 = LogisticRegression(C=4, penalty='l1',verbose=5)\n",
    "lr1.fit(X_train_os, y_train_os.ravel())\n",
    "\n",
    "LogisticRegression(C=4, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=5, warm_start=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_train.describe(include=['O'])\n",
    "\n",
    "# 将oject数据转化为int类型\n",
    "for feature in data.columns:\n",
    "    if data[feature].dtype == 'object':\n",
    "        data[feature] = pd.Categorical(data[feature]).codes # codes\t这个分类的分类代码\n",
    "\n",
    "\n",
    "选取特征数据与类别数据\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_df = data.iloc[:,data.columns != 'income_bracket']\n",
    "y_df = data.iloc[:,data.columns == 'income_bracket']\n",
    "\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "\n",
    "特征重要性评估\n",
    "在这里我们使用DecisionTreesClassifier来判断特征变量的重要性\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# fit an Extra Tree model to the data\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# 显示每个属性的相对重要性得分\n",
    "relval = tree.feature_importances_\n",
    "\n",
    "\n",
    "\n",
    "递归特征消除 (RFE)\n",
    "选取10个重要特征\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# 使用决策树作为模型\n",
    "lr = DecisionTreeClassifier()\n",
    "names = X_df.columns.tolist()\n",
    "\n",
    "# 将所有特征排序\n",
    "selector = RFE(lr, n_features_to_select = 10)\n",
    "selector.fit(X,y.ravel())\n",
    "\n",
    "print(\"排序后的特征：\",sorted(zip(map(lambda x:round(x,4), selector.ranking_), names)))\n",
    "\n",
    "# 得到新的dataframe\n",
    "X_df_new = X_df.iloc[:, selector.get_support(indices = False)]\n",
    "X_df_new.columns\n",
    "\n",
    "\n",
    "data_int=data.loc[:,['age','fnlwgt','capital.loss','hours.per.week']]\n",
    "f,ax=plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(data_int.corr(),annot=True, linewidths=.5, fmt= '.1f',ax=ax)\n",
    "\n",
    "\n",
    "len(list(set(basicwords)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets  # sklearn即scikit-learn库\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "iris = datasets.load_iris()  # 方便起见，直接使用sklearn中内置的鸢尾花数据集\n",
    "X = iris.data[:, :2]  # 为方便可视化，仅取2个特征\n",
    "y = iris.target\n",
    "\n",
    "# 展示下数据集中的数据分布\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1])\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1])\n",
    "plt.scatter(X[y == 2, 0], X[y == 2, 1])\n",
    "plt.show()\n",
    "\n",
    "# 为了检测模型的准确率，防止模型在训练集中过拟合，将数据集随机分为训练数据集和测试数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  # 样本值\n",
    "                                                    y,  # 样本对应标签\n",
    "                                                    random_state=666  # 为每次\n",
    "                                                    # 运行都得到相同的结果，种了颗随机种子\n",
    "                                                    )\n",
    "\n",
    "# 实例化一个kNN模型\n",
    "knn_clf = KNeighborsClassifier()\n",
    "# 将KNN模型在训练数据集上进行训练\n",
    "knn_clf.fit(X_train, y_train)\n",
    "# 在测试数据集上检测下模型的准确度\n",
    "accuracy = knn_clf.score(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# 再实例化一个kNN模型\n",
    "knn_clf2 = KNeighborsClassifier(n_neighbors=6, weights='distance', p=2)\n",
    "# 将该KNN模型在训练数据集上进行训练\n",
    "knn_clf2.fit(X_train, y_train)\n",
    "# 在测试数据集上检测下模型的准确度\n",
    "accuracy = knn_clf2.score(X_test, y_test)\n",
    "# 打印准确率\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "best_k = -1\n",
    "best_p = -1\n",
    "best_accuracy = 0\n",
    "\n",
    "for k in range(3, 10):\n",
    "    for p in range(1, 11):\n",
    "        # 实例化一个kNN模型, 为加快运算速度使n_jobs=-1(使用CPU所有核运算)\n",
    "        knn_clf2 = KNeighborsClassifier(n_neighbors=k, weights='distance', p=p, n_jobs=-1)\n",
    "        # 将该KNN模型在训练数据集上进行训练\n",
    "        knn_clf2.fit(X_train, y_train)\n",
    "        # 在测试数据集上检测下模型的准确度\n",
    "        accuracy = knn_clf2.score(X_test, y_test)\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_k = k\n",
    "            best_p = p\n",
    "\n",
    "# 打印最佳参数值\n",
    "print(\"最佳k值: %d, 最佳p值 : %d, 最高准确度: %f\" % (best_k, best_p, best_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "高偏差：欠拟合  高方差：过拟合\n",
    "偏差指的是模型预测值与真实值的差异，是由使用的学习算法的某些错误或过于简单的假设造成的误差。它会导致模型欠拟合，很难有高的预测准确率。\n",
    "方差指的是不同训练数据训练的模型的预测值之间的差异，它是由于使用的算法模型过于复杂，导致对训练数据的变化十分敏感，这样会导致模型过拟合，使得模型带入了过多的噪音。\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#标准化，返回值为标准化后的数据\n",
    "StandardScaler().fit_transform(iris.data)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#区间缩放，返回值为缩放到[0, 1]区间的数据\n",
    "MinMaxScaler().fit_transform(iris.data)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#归一化，返回值为归一化后的数据\n",
    "Normalizer().fit_transform(iris.data)\n",
    "\n",
    "标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。\n",
    "归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 妇女/儿童 男士标签\n",
    "child_age = 18\n",
    "def get_person(passenger):\n",
    "    age, sex = passenger\n",
    "    if (age < child_age):\n",
    "        return 'child'\n",
    "    elif (sex == 'female'):\n",
    "        return 'female_adult'\n",
    "    else:\n",
    "        return 'male_adult'\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(df[['Age', 'Sex']].apply(get_person, axis=1), columns=['person'])],axis=1)\n",
    "df = pd.concat([df,pd.get_dummies(df['person'])],axis=1)\n",
    "\n",
    "numpy.ravel() vs numpy.flatten()\n",
    "两者所要实现的功能是一致的（将多维数组降位一维），两者的区别在于返回拷贝（copy）还是返回视图（view），\n",
    "numpy.flatten()返回一份拷贝，对拷贝所做的修改不会影响（reflects）原始矩阵，\n",
    "而numpy.ravel()返回的是视图（view，也颇有几分C/C++引用reference的意味），会影响（reflects）原始矩阵。\n",
    "\n",
    "\n",
    "\n",
    "方差分析   X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "单变量特征选取  返回k个最佳特征\n",
    "单变量特征提取的原理是分别计算每个特征的某个统计指标，根据该指标来选取特征。\n",
    "SelectKBest、SelectPercentile，前者选择排名前k个的特征，后者选择排名在前k%的特征。选择的统计指标需要指定，对于regression问题，使用f_regression指标;对于classification问题，可以使用chi2或者f_classif指标。\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "X_new=SelectKBest(chi2,k=2).fit_transform(test_X,test_Y)\n",
    "False Positive Rate，假阳性率\n",
    "chi2,卡方统计量，X中特征取值必须非负。卡方检验用来测度随机变量之间的依赖关系。通过卡方检验得到的特征之间是最可能独立的随机变量，因此这些特征的区分度很高。\n",
    "\n",
    "ANOVA方差分析的 F值 来对各个特征变量打分，打分的意义是：各个特征变量对目标变量的影响权重\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,chi2\n",
    "\n",
    "target = data_train[\"Survived\"].values\n",
    "features= ['female','male','Age','male_adult','female_adult', 'child','TitleCat',\n",
    "           'Pclass','Ticket_Id','NameLength','CabinType','CabinCat', 'SibSp', 'Parch',\n",
    "           'Fare','Embarked','Surname_Numbers','Ticket_Numbers','FamilySize',\n",
    "           'Ticket_dead_women','Ticket_surviving_men',\n",
    "           'Surname_dead_women','Surname_surviving_men']\n",
    "\n",
    "train = df[0:891].copy()\n",
    "test = df[891:].copy()\n",
    "\n",
    "selector = SelectKBest(f_classif, k=len(features))\n",
    "selector.fit(train[features], target)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "indices = np.argsort(scores)[::-1]\n",
    "print(\"Features importance :\")\n",
    "for f in range(len(scores)):\n",
    "    print(\"%0.2f %s\" % (scores[indices[f]],features[indices[f]]))\n",
    "\n",
    "***********************************************************************************************************************************\n",
    "特征选择\n",
    "单变量特征选择\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.feature_selection import SelectKBest\n",
    ">>> from sklearn.feature_selection import chi2\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> X.shape\n",
    "(150, 4)\n",
    ">>> X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    ">>> X_new.shape\n",
    "(150, 2)\n",
    "\n",
    "iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "# X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "# print(X_new)\n",
    "model = SelectKBest(chi2, k=2)#选择k个最佳特征\n",
    "model.fit_transform(iris.data, iris.target)#iris.data是特征数据，iris.target是标签数据，该函数可以选择出k个特征\n",
    "print(model.scores_) # 得分\n",
    "print(model.pvalues_) # p-values\n",
    "\n",
    "\n",
    "基于树的特征选择\n",
    ">>> from sklearn.ensemble import ExtraTreesClassifier\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> X.shape\n",
    "(150, 4)\n",
    ">>> clf = ExtraTreesClassifier()\n",
    ">>> X_new = clf.fit(X, y).transform(X)\n",
    ">>> clf.feature_importances_\n",
    "array([ 0.04...,  0.05...,  0.4...,  0.4...])\n",
    ">>> X_new.shape\n",
    "(150, 2)\n",
    "\n",
    "\n",
    "上采样：\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('训练集的交易记录条数：',X_train.shape[0])\n",
    "print('测试集的交易记录条数：',X_test.shape[0])\n",
    "print('交易记录总数：',X_train.shape[0] + X_test.shape[0])\n",
    "print('上采样前，类别为‘1’的共有{}个，类别为‘0’的共有{}个。'.format(sum(y_train==1),sum(y_train==0)))\n",
    "print('------------------------')\n",
    "\n",
    "# 对训练集进行上采样处理\n",
    "smote = SMOTE(random_state=2)\n",
    "X_train_os,y_train_os = smote.fit_sample(X_train, y_train.ravel()) # ravel(): change the shape of y to (n_samples, )\n",
    "\n",
    "print('上采样后，训练集的交易记录条数：', len(X_train_os))\n",
    "print('其中，训练集X的shape:',X_train_os.shape,'，y的shape:',y_train_os.shape)\n",
    "print('交易记录总数：',X_train_os.shape[0] + X_test.shape[0])\n",
    "print('上采样后，类别为‘1’的共有{}个，类别为‘0’的共有{}个。'.format(sum(y_train_os==1),sum(y_train_os==0)))\n",
    "\n",
    "在正负样本都非常之少的情况下，应该采用数据合成的方式；SMOTE\n",
    "在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法；\n",
    "在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。\n",
    "\n",
    "\n",
    "# 3倍标准差定义异常值\n",
    "ageMean = np.mean(data_train['age'])\n",
    "ageStd = np.std(data_train['age'])\n",
    "ageUpLimit = round((ageMean + 3*ageStd),2)\n",
    "ageDownLimit = round((ageMean - 3*ageStd),2)\n",
    "print('年龄异常值上限为：{0}, 下限为：{1}'.format(ageUpLimit,ageDownLimit))\n",
    "\n",
    "# 四分位距观察异常值\n",
    "agePercentile = np.percentile(data_train['age'],[0,25,50,75,100])\n",
    "ageIQR = agePercentile[3] - agePercentile[1]\n",
    "ageUpLimit = agePercentile[3]+ageIQR*1.5\n",
    "ageDownLimit = agePercentile[1]-ageIQR*1.5\n",
    "print('年龄异常值上限为：{0}, 下限为：{1}'.format(ageUpLimit,ageDownLimit))\n",
    "print('上届异常值占比：{0} %'.format(data_train[data_train['age']>96].shape[0]*100/data_train.shape[0]))\n",
    "print('下届异常值占比：{0} %'.format(data_train[data_train['age']<8].shape[0]*100/data_train.shape[0]))\n",
    "\n",
    "data_train.loc[(data_train['Num30-59late']>=8), 'Num30-59late'] = 8\n",
    "Num30_59lateDlq = data_train.groupby(['Num30-59late'])['IsDlq'].sum()\n",
    "Num30_59lateAll = data_train.groupby(['Num30-59late'])['IsDlq'].count()\n",
    "Num30_59lateGroup = Num30_59lateDlq/Num30_59lateAll\n",
    "Num30_59lateGroup.plot(kind='bar',figsize=(10,5))\n",
    "\n",
    "Num30_59lateDf = pd.DataFrame(Num30_59lateDlq)\n",
    "Num30_59lateDf['All'] = Num30_59lateAll\n",
    "Num30_59lateDf['BadRate'] = Num30_59lateGroup\n",
    "Num30_59lateDf\n",
    "\n",
    "\n",
    "df = pd.read_csv('douban.csv', header=None, names=[\"quote\", \"score\", \"info\", \"title\", \"people\"])\n",
    "(dom1, dom2, dom3, dom4) = ([], [], [], [])\n",
    "# 清洗数据,获取电影年份及国家,增加年份列及国家列\n",
    "for i in df['info']:\n",
    "    country = i.split('/')[1].split(' ')[0].strip()\n",
    "    if country in ['中国大陆', '台湾', '香港']:\n",
    "        dom1.append(1)\n",
    "    else:\n",
    "        dom1.append(0)\n",
    "    dom2.append(int(i.split('/')[0].replace('(中国大陆)', '').strip()))\n",
    "df['country'] = dom1\n",
    "df['year'] = dom2\n",
    "# 清洗数据,建立评价人数列\n",
    "for i in df['people']:\n",
    "    dom3.append(int(i.replace('人评价', '')))\n",
    "df['people_num'] = dom3\n",
    "# 生成电影排名列表\n",
    "dom4 = [x for x in range(1, 251)]\n",
    "df['rank'] = dom4\n",
    "\n",
    "# corr()方法:计算两两相关的列,不包括NA/Null值 persion:标准相关系数\n",
    "print(df[['rank', 'score']].corr(method='pearson')\n",
    "\n",
    "\n",
    "# 生成带辅助线的散点图矩阵,hue:分类\n",
    "sns.pairplot(df[['score', 'people_num', 'year', 'country', 'rank']], hue='country', kind='reg', diag_kind='kde', size=1.5)\n",
    "\n",
    "# distplot:集合功能,kde:显示核密度估计图,fit:控制拟合的参数分布图形,本次为拟合正态分布\n",
    "sns.distplot(df.score, kde=True, fit=stats.norm)\n",
    "\n",
    "ctime = time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(info['ctime']))\n",
    "\n",
    "data['time'] = data['ctime'].apply(lambda x: x[:10])\n",
    "time = data[['time']].copy()\n",
    "time['time_comment'] = 1\n",
    "time = time.groupby(by=['time']).count()\n",
    "\n",
    "data[\"semiscore\"] = data['comment'].apply(lambda x: SnowNLP(x).sentiments)\n",
    "data['semilabel'] = data[\"semiscore\"].apply(lambda x: 1 if x>0.5 else -1)\n",
    "\n",
    "\n",
    "#词云图\n",
    "import jieba\n",
    "comment=''.join(data['comment'])\n",
    "wordlist = jieba.cut(comment, cut_all=False)\n",
    "stopwords_chinese = [line.strip() for line in open('stopwords_chinese.txt',encoding='UTF-8').readlines()]\n",
    "#过滤掉单个字\n",
    "word_list=[]\n",
    "for seg in wordlist:\n",
    "    if seg not in stopwords_chinese:\n",
    "        word_list.append(seg)\n",
    "\n",
    "word_list=pd.DataFrame({'comment':word_list})\n",
    "word_rank = word_list[\"comment\"].value_counts()\n",
    "\n",
    "from pyecharts import WordCloud\n",
    "wordcloud_chinese = WordCloud(width=1500, height=820)\n",
    "wordcloud_chinese.add(\"\", word_rank.index[0:100], word_rank.values[0:100], word_size_range=[20, 200], is_more_utils=True)\n",
    "wordcloud_chinese.render(\"comment.html\")\n",
    "\n",
    "\n",
    "图表布局 Grid\n",
    "两图结合 Overlap\n",
    "\n",
    "df['salary'] = df.salary.map({\"low\": 0, \"medium\": 1, \"high\": 2})\n",
    "\n",
    "批量梯度下降（BGD）、随机梯度下降（SGD）、小批量随机梯度下降（MSGD）\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "参数C是正则化项参数的倒数, C的数值越小, 惩罚的力度越大. penalty可选L1, L2正则化项, 默认是L2正则化.\n",
    "参数solver可选{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}这5个优化算法:\n",
    "newton-cg, lbfgs是拟牛顿法, liblinear是坐标轴下降法, sag, saga是随机梯度下降法,\n",
    "saga可以适用于L1和L2正则化项, 而sag只能用于L2正则化项.\n",
    "\n",
    "SMOTE是一种过采样算法，它构造新的小类样本而不是产生小类中已有的样本的副本。它基于距离度量选择小类别下两个或者更多的相似样本，\n",
    "然后选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声，每次处理一个属性。这样就构造了许多新数据。\n",
    "\n",
    "\n",
    "field = data.price\n",
    "Q1 = np.quantile(data[field], 0.25)\n",
    "Q3 = np.quantile(data[field], 0.75)\n",
    "deta = (Q3 - Q1) * 1.5\n",
    "data = data[(data[field] >= Q1 - deta) & (data[field] <= Q3 + deta)]\n",
    "\n",
    "k-means:\n",
    "随机生成K个聚类中心，\n",
    "内循环同时进行簇分配：看样本哪些离各自的簇中心近，并进行分配\n",
    "移动聚类中心：算出各自簇中样本的均值并把簇中心移动到该处，重复进行簇分配和移动聚类中心，直到迭代结束\n",
    "\n",
    "随机生成K个聚类中心，内循环同时进行簇分配和移动聚类中心\n",
    "\n",
    "1  随机选取K个点, 作为初始的K个聚类中心\n",
    "2  计算每个样本点到K个聚类中心的距离, 并将其分给距离最短的簇\n",
    "3  计算K个簇中所有样本点的均值, 将这K个均值作为K个新的聚类中心\n",
    "4  重复第2步和第3步, 直到聚类中心不再改变时停止算法, 输出聚类结果\n",
    "\n",
    "优点:\n",
    "1 原理简单, 计算速度快\n",
    "2 聚类效果较理想.\n",
    "缺点:\n",
    "1 K值以及初始质心对结果影响较大, 且不好把握.\n",
    "2 在大数据集上收敛较慢.\n",
    "3 由于目标函数(簇内离差平方和最小)是非凸函数, 因此通过迭代只能保证局部最优.\n",
    "4 对于离群点较敏感, 这是由于其是基于均值计算的, 而均值易受离群点的影响.\n",
    "5 由于其是基于距离进行计算的, 因此通常只能发现\"类圆形\"的聚类.\n",
    "注意:\n",
    "1 对数据异常值的处理；\n",
    "2 对数据标准化处理（x-min(x))/(max(x)-min(x)）；\n",
    "3 每一个类别的数量要大体均等；（\n",
    "4 不同类别间的特质值应该差异较大\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "K = range(1, 10)\n",
    "sse = []\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k, random_state=10)\n",
    "    km.fit(del_df)\n",
    "    sse.append(km.inertia_)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K, sse, '-o', alpha=0.7)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"簇内误差平方和(SSE)\")\n",
    "plt.show()\n",
    "\n",
    "inertias：是K-Means模型对象的属性，它作为没有真实分类结果标签下的非监督式评估指标。\n",
    "          表示样本到最近的聚类中心的距离总和。值越小越好，越小表示样本在类间的分布越集中。\n",
    "\n",
    "平行坐标图\n",
    "from pandas.plotting import parallel_coordinates\n",
    "#训练模型\n",
    "km = KMeans(n_clusters=2, random_state=10)\n",
    "km.fit(del_df)\n",
    "centers = km.cluster_centers_\n",
    "labels =  km.labels_\n",
    "customer = pd.DataFrame({'0': centers[0], \"1\": centers[1]}).T\n",
    "customer.columns = del_df.keys()\n",
    "df_median = pd.DataFrame({'2': del_df.median()}).T\n",
    "customer = pd.concat([customer, df_median])\n",
    "customer[\"category\"] = [\"customer_1\", \"customer_2\", 'median']\n",
    "#绘制图像\n",
    "plt.figure(figsize=(12, 6))\n",
    "parallel_coordinates(customer, \"category\", colormap='flag'')\n",
    "plt.xticks(rotation = 15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#将聚类后的标签加入数据集\n",
    "del_df['category'] = labels\n",
    "del_df['category'] = np.where(del_df.category == 0, 'customer_1', 'customer_2')\n",
    "customer = pd.DataFrame({'0': centers[0], \"1\": centers[1]}).T\n",
    "customer[\"category\"] = ['customer_1_center', \"customer_2_center\"]\n",
    "customer.columns = del_df.keys()\n",
    "del_df = pd.concat([del_df, customer])\n",
    "#对6类产品每年消费水平进行绘制图像\n",
    "df_new = del_df[['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen', 'category']]\n",
    "plt.figure(figsize=(18, 6))\n",
    "parallel_coordinates(df_new, \"category\", colormap='cool')\n",
    "plt.xticks(rotation = 15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "信息增益的弊端：对可取值数目较多的属性有所偏好。因为信息增益反映的是给定一个条件以后不确定性减少的程度，\n",
    "               必然是分得越细的数据集确定性更高，也就是条件熵越小，信息增益越大。\n",
    "\n",
    "1 不能处理连续特征\n",
    "2 用信息增益作为标准容易偏向于取值较多的特征\n",
    "3 不能处理缺失值\n",
    "4 容易发生过拟合问题\n",
    "\n",
    "\n",
    "信息增益和信息增益率选择最大值，基尼系数选择最小\n",
    "\n",
    "\n",
    "偏差(欠拟合)是模型所做的简化假设，使得目标函数更加容易求解\n",
    "方差(过拟合)是在给定不同训练数据集的情况下，目标函数估计值所改变的量\n",
    "\n",
    "\n",
    "Z-score：分类、聚类中使用距离来度量相似性或者使用PCA技术进行降维\n",
    "min-max：在不涉及距离度量、协方差计算、数据不符合正太分布的时。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。\n",
    "\n",
    "***********************************************************************************************************************************\n",
    "将有偏的数值特征对数化 np.log1p(train['price'])\n",
    "\n",
    "all_data = pd.concat([train, test], axis = 0, ignore_index= True)\n",
    "all_data.drop(labels = [\"price\"],axis = 1, inplace = True)\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "g1 = sns.distplot(train['price'],hist = True,label='skewness:{:.2f}'.format(train['price'].skew()),ax = ax1)\n",
    "g1.legend()\n",
    "g1.set(xlabel = 'Price')\n",
    "g2 = sns.distplot(np.log1p(train['price']),hist = True,label='skewness:{:.2f}'.format(np.log1p(train['price']).skew()),ax=ax2)\n",
    "g2.legend()\n",
    "g2.set(xlabel = 'log(Price+1)')\n",
    "plt.show()\n",
    "\n",
    "train['price'] = np.log1p(train['price'])\n",
    "# 将有偏的数值特征对数化\n",
    "num_features_list = list(all_data.dtypes[all_data.dtypes != \"object\"].index)\n",
    "for i in num_features_list:\n",
    "    if all_data[i].dropna().skew() > 0.75:\n",
    "        all_data[i] = np.log1p(all_data[i])\n",
    "\n",
    "\n",
    "train.drop(train[(train[\"sqft_living\"]>0.125)&(train[\"price\"]<20)].index,inplace=True)\n",
    "\n",
    "\n",
    "XGB重要性特征筛选\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg_model=xgb.XGBRegressor(max_depth=5,n_estimators=500,n_jobs=-1)\n",
    "reg_model.fit(X_train,Y_train)\n",
    "y_pred=reg_model.predict(X_val)\n",
    "print(np.sqrt(mean_squared_error(Y_val,y_pred)),end=' ')\n",
    "\n",
    "feature=X_train.columns\n",
    "fe_im=reg_model.feature_importances_\n",
    "print(pd.DataFrame({'fe':feature,'im':fe_im}).sort_values(by='im',ascending=False))\n",
    "\n",
    "学习台大林轩田《机器学习基石》《机器学习技法》和吴恩达《深度学习专项课程》的课程\n",
    "\n",
    "矩阵的主成分就是其协方差矩阵对应的特征向量，按照对应的特征值大小进行排序，最大的特征值就是第一主成分，其次是第二主成分，以此类推。\n",
    "总的来说，就是在数据信息损失最小的情况下，将数据的特征数量由n，通过映射到另一个空间的方式，变为k(k<n)。\n",
    "PCA主要分为三个部分。（1）生成协方差矩阵；（2）计算特征值和特征向量，并选取主成分；（3）将原始数据投影到降维的子空间中。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
