{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 级联删除数据库下的表\n",
    "drop database if exists mydb cascade;\n",
    "\n",
    "create table employee(\n",
    " id int\n",
    " ,name string\n",
    " ,hobby array<string>\n",
    " ,add map<String,string>\n",
    ")\n",
    "row format delimited fields terminated by ','\n",
    "collection items terminated by '-'\n",
    "map keys terminated by ':';\n",
    "\n",
    "# 查看基础表结构\n",
    "desc employee\n",
    "# 查看详细的表结构信息\n",
    "desc formatted employee;\n",
    "# 查看建表语句\n",
    "show create table employee;\n",
    "\n",
    "# 查看数据库的描述信息和⽂件⽬录位置路径信息；\n",
    "desc database mydatabase;\n",
    "\n",
    "# 查看表是否是管理表或者是外部表  tableType:MANAGED_TABLE 表示管理表  tableType:EXTERNAL_TABLE 表示外部表\n",
    "desc extended  table_name \n",
    "\n",
    "***********************************************************************************************************************************\n",
    "create table t1(\n",
    " id int\n",
    " ,name string\n",
    " ,hobby array<string>\n",
    " ,add map<String,string>\n",
    ")\n",
    "row format delimited fields terminated by ','\n",
    "collection items terminated by '-'\n",
    "map keys terminated by ':';\n",
    "\n",
    "# overwrite是覆盖，把overwrite换成into是追加。 \n",
    "load data local inpath '/home/hadoop/Desktop/data' overwrite into table t1;\n",
    "load data local inpath '/usr/host/data'  into table testtable;\n",
    "\n",
    "# 加载本地数据\n",
    "load data local inpath 'localpath' [overwrite] into table tablename \n",
    "# 加载hdfs数据\n",
    "load data inpath 'hdfspath'[overwrite] into table tablename\n",
    "\n",
    "\n",
    "create external table t2(\n",
    " id int\n",
    " ,name string\n",
    " ,hobby array<string>\n",
    " ,add map<String,string>\n",
    ")\n",
    "row format delimited fields terminated by ','\n",
    "collection items terminated by '-'\n",
    "map keys terminated by ':'\n",
    "location '/user/t2';\n",
    "\n",
    "load data local inpath '/home/hadoop/Desktop/data' overwrite into table t2;\n",
    "\n",
    "\n",
    "***********************************************************************************************************************************\n",
    "create table student_tmp(id INT, name STRING)\n",
    "partitioned by(academy STRING, class STRING)\n",
    "row format delimited fields terminated by ',';\n",
    "\n",
    "load data local inpath '/Users/jack/Documents/data' into table student_tmp partition(academy='physics',class='034'); \n",
    "\n",
    "# 查看表的所有分区：\n",
    "show partitions student_tmp\n",
    "show partitions employees1 partition(country='china')\n",
    "\n",
    "***********************************************************************************************************************************\n",
    "create table employees\n",
    " (\n",
    " name string,\n",
    " salary float,\n",
    " subordinated array<string>,\n",
    " deductions map<string,float>,\n",
    " address struct<street:string,city:string,state:string,zip:int>\n",
    " )\n",
    " partitioned by (country string,state string)\n",
    " row format delimited fields terminated by '\\001'\n",
    " collection items terminated by '\\002'\n",
    " map keys terminated by '\\003'\n",
    " lines terminated by \"\\n\"\n",
    " stored as textfile; # 默认\n",
    "\n",
    "# 如果⽂件数据是纯⽂本，可以使⽤ stored as textfile。如果数据需要压缩，使⽤ stored as sequencefile 。\n",
    "\n",
    "# 分区操作\n",
    "alter table employees add partition (country=\"US\",state=\"CA\");\n",
    "alter table employees drop partition(country=\"china\",state=\"Asia\");\n",
    "\n",
    "# 1，把⼀个分区打包成⼀个har包\n",
    " alter table employees archive partition (country=\"china\",state=\"Asia\")\n",
    "# 2, 把⼀个分区har包还原成原来的分区\n",
    " alter table employees unarchive partition (country=\"china\",state=\"Asia\")\n",
    "    \n",
    "# 3, 保护分区防⽌被删除\n",
    " alter table employees partition (country=\"china\",state=\"Asia\") enable no_drop\n",
    "# 4,保护分区防⽌被查询\n",
    " alter table employees partition (country=\"china\",state=\"Asia\") enable offline\n",
    "\n",
    "# 5，允许分区删除和查询\n",
    " alter table employees partition (country=\"china\",state=\"Asia\") disable no_drop\n",
    " alter table employees partition (country=\"china\",state=\"Asia\") disable offline\n",
    "\n",
    "insert overwrite/into table copy_employees partition（country=\"china\",state=\"Asia\"） \n",
    "select * from employees es where es.country=\"china\" and es.state =\"Asia\"\n",
    "\n",
    "# 修改表名\n",
    "alter table employees rename to employees_bak;\n",
    "\n",
    "\n",
    "# 外部表\n",
    "# 添加分区表\n",
    "alter table logmsgs add if not exists\n",
    "partition(year=2017,month=1,day=1) location 'logs/2017/01/01'\n",
    "partition(year=2017,month=1,day=2) location 'logs/2017/01/02'\n",
    "# 修改分区表\n",
    "alter table logmsgs partition(year=2017,month=02,day=09) set location '/usr/log/2017/02/09'\n",
    "# 增加列的信息\n",
    "alter table logmsgs add columns(app_name string comment 'Application name',session_id long comment 'The current session id');\n",
    "# 修改列的信息\n",
    "alter table logmsgs\n",
    "change column hms hours_minutes_seconds int\n",
    "comment 'The hours,minutes and seconds part of the timestamp'\n",
    "after severity;\n",
    "\n",
    "# 创建数据库，判断该数据库是否存在，并给数据库⼀个基本描述\n",
    "CREATE DATABASE IF NOT EXISTS hive2\n",
    "COMMENT \"this is ruoze database\"\n",
    "WITH DBPROPERTIES (\"creator\"=\"ruoze\", \"date\"=\"2018-08-08\");\n",
    "\n",
    "\n",
    "# Hive不⽀持⽤insert语句⼀条⼀条的进⾏插⼊操作，也不⽀持update操作。数据是以load的⽅式加载到建⽴好的表中。数据⼀旦导⼊就不可以修改。\n",
    "\n",
    "# ORDER BY与SORT BY的不同\n",
    "# ORDER BY 全局排序，只有⼀个Reduce任务;SORT BY 只在本机做排序\n",
    "\n",
    "\n",
    "# 查询数组类型,STRUCT类型,MAP类型\n",
    "SELECT name,subordinates[0],address.city,deductions[\"State Taxes\"] FROM employees\n",
    "\n",
    "***********************************************************************************************************************************\n",
    "hive> create table bucketed_user(\n",
    "    > id string,\n",
    "    > name string\n",
    "    > )\n",
    "    > clustered by(id) sorted by(name) into 4 buckets\n",
    "    > row format delimited fields terminated by '\\t' lines terminated by '\\n'\n",
    "    > stored as textfile;\n",
    "\n",
    "insert overwrite table bucketed_user select name,addr from testtext;\n",
    "\n",
    "alter table tablename change column c1 c2 int\n",
    "# 修改分隔符\n",
    "alter table city set serdeproperties('field.delim'='\\t');\n",
    "# 修改location \n",
    "alter table city set location 'hdfs://hadoop1:9000/location';\n",
    "# 内部表转外部表\n",
    "alter table table_name set TBLPROPERTIES('EXTERNAL'='TRUE');\n",
    "# 外部表转内部表\n",
    "alter table table_name set TBLPROPERTIES('EXTERNAL'='FALSE');\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select 1 from lxw_dual where 'football' like 'foot%';\n",
    "select 1 from lxw_dual where 'football' like 'foot____';\n",
    "select 1 from lxw_dual where NOT 'football' like 'fff%';\n",
    "\n",
    "# JAVA 的 LIKE 操作\n",
    "select 1 from lxw_dual where 'footbar’ rlike '^f.*r$’;\n",
    "# 判断一个字符串是否全为数字\n",
    "select 1 from lxw_dual where '123456' rlike '^\\\\d+$';\n",
    "select 1 from lxw_dual where '123456aa' rlike '^\\\\d+$';\n",
    "\n",
    "# 时间戳转日期\n",
    "select from_unixtime(1323308943,'yyyyMMdd') from lxw_dual;\n",
    "# 日期转时间戳\n",
    "select unix_timestamp('20111207 13:01:03','yyyyMMdd HH:mm:ss') from lxw_dual;\n",
    "\n",
    "select datediff('2012-12-08','2012-05-09') from lxw_dual;\n",
    "select date_add('2012-12-08',10) from lxw_dual;\n",
    "select date_sub('2012-12-08',10) from lxw_dual;\n",
    "\n",
    "select if(1=2,100,200) from lxw_dual;\n",
    "select COALESCE(null,'100','50') from lxw_dual;  # 100\n",
    "\n",
    "select reverse(abcedfg’) from lxw_dual;  # gfdecba\n",
    "select concat(‘abc’,'def’,'gh’) from lxw_dual;  # abcdefgh\n",
    "select concat_ws(',','abc','def','gh') from lxw_dual;  # abc,def,gh\n",
    "select substring('abcde',-2,2) from lxw_dual;  # de\n",
    "select regexp_replace('foobar', 'oo|ar', '') from lxw_dual;  # fb\n",
    "select regexp_extract('foothebar', 'foo(.*?)(bar)', 2) from lxw_dual;  # bar\n",
    "select get_json_object(request_uri_dmalog_show_post_id,'$.pn') from ods_rtm_app_ev \n",
    "\n",
    "select space(10) from lxw_dual;  # 空格字符串函数\n",
    "select repeat('abc',5) from lxw_dual;  # abcabcabcabcabc\n",
    "select lpad('abc',10,'td') from lxw_dual;  # tdtdtdtabc\n",
    "select rpad('abc',10,'td') from lxw_dual;  # abctdtdtdt\n",
    "select split('abtcdtef','t') from lxw_dual;  # [\"ab\",\"cd\",\"ef\"]\n",
    "\n",
    "# 集合查找函数:  find_in_set(string str, string strList)\n",
    "# 说明: 返回 str 在 strlist 第一次出现的位置，strlist 是用逗号分割的字符串。如果没有找到该 str 字符，则返回 0\n",
    "hive> select find_in_set('ab','ef,ab,de') from lxw_dual;\n",
    "2\n",
    "hive> select find_in_set('at','ef,ab,de') from lxw_dual;\n",
    "0\n",
    "\n",
    "\n",
    "hive> create table lxw_test as select array(\"tom\",\"mary\",\"tim\") as t from lxw_dual;\n",
    "hive> select t[0],t[1],t[2] from lxw_test;\n",
    "tom mary tim\n",
    "\n",
    "hive> Create table lxw_test as select map('100','tom','200','mary') as t from lxw_dual;\n",
    "hive> select t['200'],t['100'] from lxw_test;\n",
    "mary tom\n",
    "\n",
    "hive> create table lxw_test as select struct('tom','mary','tim') as t from lxw_dual;\n",
    "hive> describe lxw_test;\n",
    "t struct<col1:string,col2:string,col3:string>\n",
    "hive> select t.col1,t.col3 from lxw_test;\n",
    "tom tim  \n",
    "\n",
    "\n",
    "# Hive\n",
    "# Hive中实现group_concat功能（不用udf） \n",
    "# 注意:collect_set 只能返回不重复的集合 若要返回带重复的要用collect_list\n",
    "SELECT id,concat_ws(',', collect_set(name)) FROM t GROUP BY id;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put [local 文件] [目标目录]\n",
    "hadoop fs -put xielaoshi /data/\n",
    "\n",
    "\n",
    "\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive分区和分桶区别\n",
    "1.分桶随机分割数据库，分区是非随机分割数据库。因为分桶是按照列的哈希函数进行分割的，相对比较平均；\n",
    "  而分区是按照列的值来进行分割的，容易造成数据倾斜。\n",
    "2.分桶是对应不同的文件（细粒度），分区是对应不同的文件夹（粗粒度）。\n",
    "  桶是更为细粒度的数据范围划分，分桶的比分区获得更高的查询处理效率，使取样更高效。\n",
    "3.注意：普通表（外部表、内部表）、分区表这三个都是对应HDFS上的目录，桶表对应是目录里的文件\n",
    "\n",
    "get_json_object(userinfo,'$.name') \n",
    "json_tuple(userinfo,'name','age')\n",
    "\n",
    "select id,b.name,b.age,b.sex\n",
    "from default.userinfo\n",
    "lateral view json_tuple(userinfo,'name','age','sex') b as name,age,sex;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
